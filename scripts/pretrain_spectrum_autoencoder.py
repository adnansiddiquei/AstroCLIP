import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from astroclip.transforms import (
    Permute,
    ExtractKey,
)
from astroclip.augmentations import SpectrumNoising
from astroclip.utils import load_config, download_desi_dataset
from astroclip.blocks import SpectrumEncoder, SpectrumDecoder
from astroclip.models import SpectrumAutoEncoder
import os
import pytorch_lightning as L


def main():
    # load the config file
    config = load_config()

    cache_dir = config['cache_dir']
    output_dir = config['output_dir']

    if not os.path.exists(cache_dir):
        raise FileNotFoundError(f'Cache directory {cache_dir} does not exist.')

    if not os.path.exists(output_dir):
        raise FileNotFoundError(f'Cache directory {output_dir} does not exist.')

    # Load the dataset, if the dataset is not already in the cache dir it'll be downloaded
    dataset = download_desi_dataset(cache_dir)

    # load the observed spectra standard deviation which is generated by compute_observed_spectra_std_dev.py
    try:
        observed_spectra_std_dev = torch.load(
            f'{output_dir}/observed_spectra_std_dev.pt'
        )
    except FileNotFoundError:
        raise FileNotFoundError(
            f'Observed spectra standard deviation file not found in {output_dir}. '
            f'Please run compute_observed_spectra_std_dev.py first.'
        )

    # Define all the spectrum transforms and augmentations
    spectrum_pre_transforms = nn.Sequential(
        ExtractKey('spectrum'),
        Permute([0, 2, 1]),  # Change to [batch_size, channel, spectrum_length]
    )

    spectrum_augmentations = nn.Sequential(
        SpectrumNoising(observed_spectra_std_dev),
    )

    # define the model
    model = SpectrumAutoEncoder(
        encoder=SpectrumEncoder(),  # default parameters are already as required.
        decoder=SpectrumDecoder(),  # as above
        pre_transforms=spectrum_pre_transforms,
        augmentations=spectrum_augmentations,
    )

    batch_size = config['autoencoder']['batch_size']
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=config['dataloader_workers'],
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=config['dataloader_workers'],
    )

    trainer = L.Trainer(
        max_epochs=config['autoencoder']['max_epochs'],
        accelerator='auto',
        devices='auto',
    )

    # Train the model
    trainer.fit(model, train_loader, val_loader)


if __name__ == '__main__':
    main()
