# import torch
# import torch.nn as nn
# from torch.utils.data import DataLoader
# from astroclip.transforms import Permute, ExtractKey, Standardize
# from astroclip.augmentations import SpectrumNoising
# from astroclip.utils import load_config, download_desi_dataset
# from astroclip.blocks import SpectrumEncoder, SpectrumDecoder
# from astroclip.models import AutoEncoder
# import os
# import pytorch_lightning as L
# from pytorch_lightning.loggers import WandbLogger
# from pytorch_lightning.callbacks import ModelCheckpoint
# import argparse
#
#
# def main():
#     # load the config argument to decide which config header to load
#     parser = argparse.ArgumentParser()
#     parser.add_argument(
#         '--config',
#         type=str,
#         required=False,
#         help='Which config to use.',
#         default='local',
#     )
#     args = parser.parse_args()
#
#     # load the config file
#     config = load_config(args.config)
#
#     cache_dir = config['cache_dir']
#     output_dir = config['output_dir']
#
#     wandb_logger = WandbLogger(
#         log_model='all', project='AstroCLIP', name='spectrum_autoencoder'
#     )
#
#     if not os.path.exists(cache_dir):
#         raise FileNotFoundError(f'Cache directory {cache_dir} does not exist.')
#
#     if not os.path.exists(output_dir):
#         raise FileNotFoundError(f'Cache directory {output_dir} does not exist.')
#
#     model_checkpoints_dir = os.path.join(output_dir, 'autoencoder_checkpoints')
#     if not os.path.exists(model_checkpoints_dir):
#         os.makedirs(model_checkpoints_dir)
#
#     # Load the dataset, if the dataset is not already in the cache dir it'll be downloaded
#     dataset = download_desi_dataset(cache_dir)
#
#     # load the observed spectra standard deviation which is generated by compute_observed_spectra_std_dev.py
#     try:
#         observed_spectra_std_dev = torch.load(
#             f'{output_dir}/observed_spectra_std_dev.pt'
#         )
#     except FileNotFoundError:
#         raise FileNotFoundError(
#             f'Observed spectra standard deviation file not found in {output_dir}. '
#             f'Please run compute_observed_spectra_std_dev.py first.'
#         )
#
#     # Define all the spectrum transforms and augmentations
#     spectrum_pre_transforms = nn.Sequential(
#         ExtractKey('spectrum'),
#         Permute([0, 2, 1]),  # Change to [batch_size, channel, spectrum_length]
#     )
#
#     spectrum_augmentations = nn.Sequential(
#         SpectrumNoising(observed_spectra_std_dev),
#     )
#
#     spectrum_post_transforms = nn.Sequential(
#         Standardize(return_mean_and_std=False),
#     )
#
#     # define the model
#     model = AutoEncoder(
#         encoder=SpectrumEncoder(),  # default parameters are already as required.
#         decoder=SpectrumDecoder(),  # as above
#         pre_transforms=spectrum_pre_transforms,
#         augmentations=spectrum_augmentations,
#         post_transforms=spectrum_post_transforms,
#     )
#
#     batch_size = config['autoencoder']['batch_size']
#     train_dataset = dataset['train']
#     val_dataset = dataset['test']
#
#     num_workers = max(os.cpu_count(), 1)
#
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=batch_size,
#         shuffle=True,
#         num_workers=num_workers,
#     )
#     val_loader = DataLoader(
#         val_dataset,
#         batch_size=batch_size,
#         shuffle=False,
#         num_workers=num_workers,
#     )
#
#     trainer = L.Trainer(
#         max_epochs=config['autoencoder']['max_epochs'],
#         accelerator='auto',
#         devices='auto',
#         logger=wandb_logger,
#         callbacks=[
#             ModelCheckpoint(
#                 dirpath=model_checkpoints_dir,
#                 filename='autoencoder-{epoch:02d}-{val/loss:.2f}',
#                 monitor='val/loss',
#                 mode='min',
#             )
#         ],
#     )
#
#     # Train the model
#     trainer.fit(model, train_loader, val_loader)
#
#
# if __name__ == '__main__':
#     main()
