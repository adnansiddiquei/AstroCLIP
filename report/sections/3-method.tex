%! Author = adnansiddiquei
%! Date = 04/06/2024

\section{Method}\label{sec:method}
Assuming that galaxy spectra and galaxy images can be viewed as different physical representations of the same underlying
object, reasonably, one can assume that there is enough mutual information within the two modalities to be able to effectively
embed them into a shared latent space.
As such, the objective of this piece of work was to reproduce the results of the original AstroCLIP paper~\citep{astroclip}
which implemented a multi-modal contrastive learning approach to embed galaxy spectra and galaxy images into a shared embedding
space.
This was done using 2 pretrained convolutional spectrum and image embedders, and then fine-tuning the model using contrastive
learning under the InfoNCE loss~\citep{oord2019}, as per Equation~\eqref{eq:infonce}, to learn a shared 128-dimensional embedding space for the two modalities.

\begin{equation}
\label{eq:infonce}
    \mathcal{L}(\mathbf{X}, \mathbf{Y}) = - \frac{1}{K} \sum_{i=1}^K \log \frac{\exp(S_C(\mathbf{x}_i, \mathbf{y}_i) / \tau)}{\sum_{j}^K \exp(S_C(\mathbf{x}_i, \mathbf{y}_j) / \tau)}.
\end{equation}

\subsection{Implementation}\label{subsec:implementation}
Our implementation did not differ too greatly from the original AstroCLIP implementation~\citep{astroclip}.
We created our AstroCLIP model with a pretrained image and spectrum embedder, and then fine-tune the model using contrastive
learning under the InfoNCE loss to learn a shared 128-dimensional embedding space for the two modalities.
To acquire the pretrained embedders, the fine-tuned embedders or the dataset used, we refer the reader to the referenced
literature or to our codebase which contains all the necessary details.

\paragraph{Spectrum Embedder} We acquired a pre-trained spectrum embedder from the works of~\cite{liang2023} who used
a convolutional autoencoder to learn a 6-dimensional embedding for galaxy spectra using the Bright Galaxy Survey of the
DESI Early Data Release~\citep{desiearly2023}, for precise details on architecture and training, see the original paper.
The encoder consisted of a series of convolutional layers followed by 4 fully-connected layers.
The last 3 of these layers were deleted and replaced with a single fully-connected layer to project the output of the first
layer into a 128-dimensional embedding\footnote{That is to say, in the edited model, the first fully-connected layer took
256 inputs and output 128-dims into the second layer which also output 128-dims.
The first layer contained pre-trained weights, the second layer was randomly initialised.}.
The entire model contained 3.2M parameters, all of which were set as trainable parameters, differing to the implementation
by~\cite{astroclip} where the authors set only the final fully-connected layers of their pre-trained transformer-based spectral
embedder as trainable.
The spectra were normalised in the same way as done by~\cite{liang2023}\footnote{The spectra were normalised over their median
flux in the rest-frame wavelength range of $[5300, 5850]$, where the rest-frame spectra were computed by assuming all
spectra were redshifted by exactly $z=0.8$.} and then noise was added to the spectra using a
similar technique as utilised by~\cite{astroclip}\footnote{Noise added to the spectra at any point was Gaussian and scaled
by the distribution of values at any given flux measurement.
Measurements at wavelengths with more variance therefore had more noise added.}.

\paragraph{Image Embedder} The image embedder we used was the same as the one used by~\cite{astroclip}, courtesy of
\cite{stein2021}, see Section~\eqref{sec:original-paper} for further details.
However, unlike~\cite{astroclip} who only fine-tuned the fully-connected layers, we once again chose to set all 28.0M parameters
of the model as trainable.
The images were transformed and augmented using the same techniques used in the original AstroCLIP paper; the 3-channel (g, r, z)
152x152 pixel images were augmented with Gaussian noise, rolled, rotated and flipped randomly before being cropped to the central
96x96 pixels.
This was then converted from (g, r, z) to RGB using the same scaling method utilised by~\cite{stein2021}.
The conversion to RBG was relatively arbitrary and did not change meaning of the image, but was done to ensure the images
were in the same format as expected by the pre-trained image embedder.

\paragraph{Data} We use the same dataset as curated and helpfully provided by \cite{astroclip}, see Section~\eqref{sec:original-paper}
for further details.
This consists of 197,976 galaxy spectra and image pairs, along with their redshift measurements.
The data was then further pre-processed to drop any galaxies with spectra outside the range $[0, 0.8]$ which was in line with the
pre-training of the spectrum embedder, but is a larger range than utilised by \cite{astroclip} who used $[0, 0.6]$ in their final results.
Additionally, some of the galaxy spectra in the dataset were completely flat (exactly 0 flux across all wavelengths)
and so these galaxies were dropped from the dataset.
This resulted in a total of 3,241 (1.6\%) image-spectra pairs being removed in the pre-processing step.
The dataset was then split into a training and validation set, with 80\% of the data used for training and 20\% for
validation; all results displayed in this paper were performed on the validation set.

\paragraph{Training} ...
