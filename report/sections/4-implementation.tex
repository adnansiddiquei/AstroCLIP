%! Author = adnansiddiquei
%! Date = 28/06/2024

\section{AstroCLIP Implementation}\label{sec:astroclip-implementation}
Our AstroCLIP model was implemented using a pretrained 2D convolutional image embedder courtesy of~\cite{stein2021}
and a pretrained 1D spectrum embedder courtesy of~\cite{liang2023}.
These were placed into our unified AstroCLIP model and then fine-tuned (with all the weights set as trainable) using
contrastive learning under the InfoNCE loss as per Equation~\eqref{eq:infonce}, to learn the shared low-dimensional
embedding space.
Embeddings were projected into a variety of dimensions: (8, 16, 32, 64, 128, 256, 512) to explore the effect of the
dimensionality of the embedding space on the performance of the model, with~\cite{astroclip} using a 512 dimensional
embedding for their final results.
Below we give a brief overview of the pretraining strategies for the image and spectrum embedders, but for a
comprehensive report we refer the reader to the original papers.

\subsection{Image Embedder}\label{subsec:image-embedder}
\cite{stein2021} trained a galaxy image embedder using a self-supervised method for the purposes of image based similarity search.
The image embedder was trained via the MoCo v2 framework~\citep{moco2020, mocov22020} on a sample of 42 million galaxies
from the DESI Legacy Survey Data Release 9~\citep{desilegacy2018}.
The encoder architecture used was a ResNet-50 architecture.
The images underwent a variety of augmentations (galactic extinction, random rotation, blurring, noising) before
being cropped to the central 96x96 pixels.
The loss function used was the same InfoNCE loss we used in our AstroCLIP model as given in Equation~\eqref{eq:infonce},
and the similarity measure used was also the cosine similarity function as given in Equation~\eqref{eq:cosine-similarity}.
Their model yielded very promising results, and we refer the reader to the original paper for a thorough discussion
on the trained model.
The final fully-connected layers of this model contained 3 layers with 2048, 2048 and 128 units respectively.
The final 128 unit layer was adjusted as required to produce the output dimensionality we desired, but otherwise all
weights and biases were initiated as per the pretrained model.
This yielded between 27.7M and 28.8M trainable parameters depending on the output dimensionality of the model.

\cite{astroclip} pre-train a vision transformer model for their image embedder, we refer the reader to the original paper
for a more detailed discussion on the architecture and pre-training strategy for this.

\subsection{Spectrum Embedder}\label{subsec:spectrum-embedder}
\cite{liang2023} trained a galaxy spectrum autoencoder using galaxy spectra from the DESI Early Data Release~\citep{desiearly2023},
focusing primarily on the Bright Galaxy Survey (BGS) (for precise details on the spectral dataset used, see the original paper).
The autoencoder architecture, named SPENDER, was proposed by~\cite{melchior2022} and encoded the spectra into a 6-dimensional
embedding space.
For our AstroCLIP model, we used the encoder part of the SPENDER model which consisted of a series of 1D convolutional layers
followed by a series of fully-connected layers to bring the dimensionality down to 6.
We reduced the fully-connected layers to 3 layers with 256, $D_{-1}$ and $D_{out}$ units respectively, for any desired
output dimensionality $D_{out}$.
The first 2 fully-connected layers in the pre-trained SPENDER encoder originally had a dimensionality of 256 and 128 units
and as such, for $D_{out} \in [8, 16, 32, 64, 128]$, we retained $D_{-1}=128$ and so the weights and biases of the
first 2 fully-connected layer were retained with the output layer being initialised from a Gaussian distribution.
For $D_{out} \in [256, 512]$, $D_{-1}$ was set to 256 and the entire fully-connected part of the model was retrained
from Gaussian initialisation.
The convolutional layers were initialised from the pre-trained weights of the SPENDER model.
This yielded between 3.1M to 3.3M trainable parameters, depending on the output dimensionality of the model.

\cite{astroclip} once again use a transformer architecture for their spectrum encoder, and we refer the reader to the original
paper for a more detailed discussion on the architecture and pre-training strategy for this.
