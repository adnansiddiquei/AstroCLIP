%! Author = adnansiddiquei
%! Date = 05/03/2024

% Preamble
\documentclass[a4paper,11pt]{article}
\pdfoutput=1

% Packages
%\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue, pdfborder={0 0 0}]{hyperref}
\usepackage{url}
\usepackage{jcappub}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{roboto}
\usepackage{subcaption}
\usepackage{blindtext}
\usepackage{seqsplit}
\usepackage[nottoc]{tocbibind}
\usepackage{siunitx}
\DeclareSIUnit\angstrom{\text {Ã…}}

%\newcommand{\inlinecode}[1]{\lstinline{#1}}
%\newcommand{\inlinecode}[1]{\texttt{#1}}
\newcommand{\inlinecode}[1]{\texttt{\seqsplit{#1}}}
\lstset{basicstyle=\fontfamily{pcr}\selectfont}


\title{\boldmath Reproducing AstroCLIP: A Cross-Modal Foundation Model for Galaxies}

% %simple case: 2 authors, same institution
\author{Adnan Siddiquei}
\affiliation{University of Cambridge}

% e-mail addresses: one for each author, in the same order as the authors
\emailAdd{as3438@cam.ac.uk}
\note{Word Count: 4054 (including figure captions).}


\begin{document}
    \abstract{We present a reproduction of the AstroCLIP model by \cite{astroclip}, a cross-modal foundation model for galaxies.
    AstroCLIP is a single cross-modal model that can embed galaxy spectra and images into a shared low-dimensional
    embedding space.
    AstroCLIP discovers a meaningful representation of the data by training the model using contrastive training under
    the InfoNCE loss on the Dark Energy Spectroscopic Instrument (DESI) spectra and images from its corresponding
    DESI Legacy Imaging Survey.
    These cross-modal embeddings can be used for a variety of downstream tasks and we reproduce a subset of the downstream
    tasks from the original paper to assess the performance of our AstroCLIP model, including (1) in-modal and cross-modal
    similarity search and (2) zero-shot in-modal and cross-modal redshift prediction.
    Our approach to this reproduction differs from the original authors in that we utilise pre-trained convolutional
    image~\citep{stein2021} and spectrum~\citep{liang2023} embedders as opposed to the transformer architectures used
    by \cite{astroclip}.
    We also explore the performance of the model in the downstream tasks as the embedding dimensionality is varied
    through $[8, 16, 32, 64, 128, 256, 512]$ dimensions.
    We find generally equivalent performance to the original paper, and show that even low-dimensional embeddings are
    able to perform well in the downstream tasks.
    We outperform the original authors in the in-modal photometric redshift prediction task with a 128-dimensional
    embedding compared to their 512-dimensional embedding, but their transformer architecture outperforms our convolutional
    architecture in the spectroscopic redshift prediction task.
    }

    \maketitle
    \flushbottom


    \input{sections/1-introduction}
    \input{sections/2-original-paper}
    \input{sections/3-data}
    \input{sections/4-implementation}
    \input{sections/5-training}
    \input{sections/6-results}
    \input{sections/7-conclusion}

    \clearpage

%    \nocite{*}
    \bibliographystyle{apalike}
    \bibliography{main}
    \clearpage

    %\appendix

    %\section{Appendix A}\label{app:app-a}
    %Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse eget urna laoreet, elementum tellus nec, dapibus dui.
\end{document}
